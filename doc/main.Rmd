---
title: "ATE Estimation using Full Propensity Matching (implemented from scratch) and Linear Propensity Scoring"
author: "Amir Idris"
output: html_notebook
---


```{r}
if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("MatchIt")){
  install.packages("MatchIt")
}
if(!require("lmtest")){
  install.packages("lmtest")
}
if(!require("sandwich")){
  install.packages("sandwich")
}
if(!require("boot")){
  install.packages("boot")
}
if(!require("survival")){
  install.packages("survival")
}
if(!require("optmatch")){
  install.packages("optmatch")
}
if(!require("glmnet")){
  install.packages("glmnet")
}
if(!require("glmtree")){
  install.packages("glmtree")
}
if(!require("fastAdaboost")){
  install.packages("fastAdaboost")
}
if(!require("gbm")){
  install.packages("gbm")
}
if(!require("rpart")){
  install.packages("rpart")
}

library(dplyr)
library(MatchIt)
library(lmtest)
library(sandwich)
library(boot)
library(survival)
library(optmatch)
library(glmnet)
library(gbm)
library(glmtree)
library(fastAdaboost)
library(rpart)
```

First, let's load the data and summarize it.
```{r}
lowdim_data <- read.csv("../data/lowDim_dataset.csv")
highdim_data <- read.csv("../data/highDim_dataset.csv")

print(dim(lowdim_data))
print(dim(highdim_data))
```

We can see that the high-dimensional data has an order of magnitude more dimensions, and four times the data as compared to the low dimensional data. Let's view a couple rows

```{r}
head(lowdim_data)
head(highdim_data)
```

First, we must train models to estimate the propensity scores. We have five models assigned to us:
P1: Logistic Regression
P2: L1 Penalized Logistic Regression
P3: L2 penalized logistic regression
P4: Regression trees
P5: Boosted stumps
**ToDo: Add P4 and P5
```{r}
# Model selector method
source("../lib/LR_propensity_est.R")
model_selector <- function(data, mode = 1){
  if (mode == 1) {
    return(lr_propensity_model(data))
  } else if (mode == 2){
    return(lr_propensity_model(data, mode = "lasso"))
  } else if (mode == 3){
    return(lr_propensity_model(data, mode = "ridge"))
  } else if (mode == 4){
    return(lr_propensity_model(data, mode = "tree"))
  } else if (mode == 5){
    return(lr_propensity_model(data, mode = "stump"))
  }
}
```


```{r}
# P1-P3
m <- 5 # number of models
lowdim_models <- list()
highdim_models <- list()
lowdim_model_times <- list()
highdim_model_times <- list()
for(i in 1:m){
  print(i)
  lowdim_model_times[[i]] <- system.time({lowdim_models[[i]] <- model_selector(lowdim_data, mode = i);})
}
for(i in 1:m){
  highdim_model_times[[i]] <- system.time({highdim_models[[i]] <- model_selector(highdim_data, mode = i);})
}
#lowdim_p1_time <- system.time({lowdim_model <- lr_propensity_model(lowdim_data);})
#highdim_p1_time <- system.time({highdim_model <- lr_propensity_model(highdim_data);})
```

Then, we'll estimate propensity scores, and transform them to use linear propensity distance.
**ToDO: Add method for getting prop scores for P4 and P5
```{r}
lowdim_prop_scores <- list()
highdim_prop_scores <- list()
lowdim_score_times <- list()
highdim_score_times <- list()

for(i in 1:m){
  #print(length(lr_propensity(lowdim_data, lowdim_models[[i]], mode = i)))
  lowdim_score_times[[i]] <- system.time({lowdim_prop_scores[[i]] <- logit(lr_propensity(lowdim_data, lowdim_models[[i]], mode = i));})
}
for(i in 1:m){
  highdim_score_times[[i]] <- system.time({highdim_prop_scores[[i]] <- logit(lr_propensity(highdim_data, highdim_models[[i]], mode = i));})
}

#lowdim_p1scores_time <- system.time({lowdim_prop_scores <- lr_propensity(lowdim_data, lowdim_model);})
#highdim_p1scores_time <- system.time({highdim_prop_scores <- lr_propensity(highdim_data, highdim_model);})

#lowdim_prop_scores <- logit(lowdim_prop_scores)
#highdim_prop_scores <- logit(highdim_prop_scores)
```




Now, we'll proceed with the matching
```{r}
lowdim_data_match <- subset(lowdim_data, select = -c(Y))
highdim_data_match <- subset(highdim_data, select = -c(Y))
lowdim_matches <- list()
highdim_matches <- list()
lowdim_match_times <- list()
highdim_match_times <- list()

for(i in 1:m){
  lowdim_match_times[[i]] <- system.time({lowdim_matches[[i]] <- matchit(A ~ ., data = lowdim_data_match, method = "full", distance = lowdim_prop_scores[[i]], estimand = "ATE" );})
}
for(i in 1:m){
  highdim_match_times[[i]] <- system.time({highdim_matches[[i]] <- matchit(A ~ ., data = highdim_data_match, method = "full", distance = highdim_prop_scores[[i]], estimand = "ATE" );})
}

lowdim_S_time <- system.time({m.out.low <- matchit(A ~ ., data = lowdim_data_match, method = "full", distance = lowdim_prop_scores, estimand = "ATE" );})
#highdim_S_time <- system.time({m.out.high <- matchit(A ~ ., data = highdim_data_match, method = "full", distance = highdim_prop_scores, estimand = "ATE" );})

# Example matchings
lowdim_matches[[1]]
highdim_matches[[1]]
```

We obtain the matched datasets, and bind our outcome Y back to them
```{r}
md.low <- match.data(m.out.low)
Y.low <- lowdim_data$Y
md.low <- as.data.frame(cbind(Y.low, md.low))

md.high <- match.data(m.out.high)
Y.high <- highdim_data$Y
md.high <- as.data.frame(cbind(Y.high, md.high))
```

Finally, we estimate our ATEs
```{r}
fit.low <- lm(Y.low ~ A, data = md.low, weights = weights)
coeftest(fit.low, vcov. = vcovCL, cluster = ~subclass)

fit.high <- lm(Y.high ~ A, data = md.high, weights = weights)
coeftest(fit.high, vcov. = vcovCL, cluster = ~subclass)

ATE.low <- summary(fit.low)$coefficients["A", "Estimate"]
ATE.high <- summary(fit.high)$coefficients["A", "Estimate"]
```
We can see that both estimates have decently low p-values. Although the p-value for the low-dimensional case is above 0.05, our standard alpha-level, it is close enough that we can proceed with caution that it is significantly different from zero.


How do our ATE estimates compare to the real ones?
```{r}
# Real ATEs
lowdim_ATE_real <- 2.0901
highdim_ATE_real <- -54.8558

#SDs of outcomes, so we can normalize our difference in estimation
sd.low <- sd(Y.low)
sd.high <- sd(Y.high)

ATE_diff.low <- (ATE.low - lowdim_ATE_real)/sd.low
ATE_diff.high <- (ATE.high - highdim_ATE_real)/sd.high

ATE_diff.low
ATE_diff.high
```

Not bad! 

Now, let's take a look at the time our algorithm took:
```{r}
cat("Low Dimensional Dataset:")
cat("Time to train LR model=", lowdim_p1_time[3], "s \n")
cat("Time to estimate propensity scores=", lowdim_p1scores_time[3], "s \n")
cat("Time to obtain full matching over data=", lowdim_S_time[3], "s \n")

cat("\n")

cat("High Dimensional Dataset:")
cat("Time to train LR model=", highdim_p1_time[3], "s \n")
cat("Time to estimate propensity scores=", highdim_p1scores_time[3], "s \n")
cat("Time to obtain full matching over data=", highdim_S_time[3], "s \n")
```

As we can see with quadruple the data, the full matching algorithm took about 16.781 times longer for the high-dimensional dataset, and the other components of the process were negligible in comparison. So, we can hypothesis that we have an O(n^2) time complexity. 


References
- https://cran.r-project.org/web/packages/MatchIt/vignettes/estimating-effects.html#after-full-matching








