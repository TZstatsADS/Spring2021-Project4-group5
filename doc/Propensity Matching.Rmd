

## 4. Propensity Matching

The distance of Propensity Score is defined as:

$$D_{ij} = |e_i - e_j|$$

where $e_k$ is the propensity score for individual $k$.

We set up thresholds for matching and make pairs for data point in different groups which have distance below the threshold. Thus as the threshold increases, more pairs are matched, and it will converge to all data matched when threshold comes to 100 percent.

### 4.1 Distance calculated
```{r, warning=FALSE}
dist_mat <- function(li){
  glm1.fit <- li$l
  glm2.fit <- li$h
  n1 <- length(glm1.fit)
  dt1 <- matrix(0,nrow = n1, ncol = n1) 
  for (i in 1:(n1-1)){
    dt1[i,i] <- 1
    for (j in (i+1):n1){
      dt1[i,j] <- abs(glm1.fit[i] - glm1.fit[j])
      dt1[j,i] <- dt1[i,j]
    }
  }
  
  
  
  n2 <- length(glm2.fit)
  dt2 <- matrix(0,nrow = n2, ncol = n2) 
  for (i in 1:(n2-1)){
    dt2[i,i] <- 1
    for (j in (i+1):n2){
      dt2[i,j] <- abs(glm2.fit[i] - glm2.fit[j])
      dt2[j,i] <- dt2[i,j]
    }
  }
  
  return(list(lm=dt1,hm=dt2))
  
}
dist_mat_list <- lapply(p_score_list,dist_mat)
end_time <- Sys.time()
tm <- end_time - start_time
cat("Time for Preparing is:", tm, "seconds.")
```

### 4.2 Propensity Score Matching Function

```{r, warning=FALSE}
cal_neighbour <- function(index,df,thresh,y,A){
  dt_vec <- df[index,]
  ind_vec <- which(dt_vec<thresh)
  ind_final <- ind_vec[A[index]!=A[ind_vec]]
  
  if (length(ind_final) == 0){
    return(NA)
  }
  else{
    return(list(mean(y[ind_final]),ind_final))
  }
  
}
```

### 4.3 Matching Low-Dim

```{r, warning=FALSE}
seq = 10:200/10000
start_time <- Sys.time()
get_ate_pair <- function(ind){
  dt1 <- dist_mat_list[[ind]]$lm
  a <- as.vector(dt1)
  
  ATE_low <- vector("double")
  pairs_low <- vector("double")
  for (percentage in seq){
    threshold <- quantile(a,percentage)
    
    n1_vec <- 1:nrow(dt1)
    list_1 <- lapply(n1_vec, cal_neighbour, df = dt1, thresh = threshold, y = ly, A = ltr)
    mean_list_1 <- lapply(n1_vec, function(x) unlist(list_1[[x]][1]))
    mean_cal_1 <- unlist(mean_list_1)
    neighbour_list_1 <- lapply(n1_vec, function(x) unlist(list_1[[x]][2]))
    
    df_1 <- (data.frame(Y = ly, A = ltr)
             %>%mutate(ind = row_number())
             %>%mutate(AAA = neighbour_list_1)
             %>%mutate(mean_cal = mean_cal_1)
             %>%filter(!is.na(mean_cal))
             %>%mutate(ATE = (Y-mean_cal)*ifelse(A==0,-1,1))
    )
    
    ATE_low <- append(ATE_low,mean(df_1$ATE))
    pairs_low <- append(pairs_low,sum(!is.na(unlist(neighbour_list_1)))/2)
  }
  
  return(list(ate=ATE_low,pair=pairs_low))
}
ind_mat <- 1:4
low_list <- lapply(ind_mat,get_ate_pair)
end_time <- Sys.time()
tm <- end_time - start_time
cat("Time for Propensity Matching Low_Dim is:", tm, "seconds.")
```

### 4.4 Matching High-Dim

```{r, warning=FALSE}
start_time <- Sys.time()
seq = 10:200/10000
get_ate_pair <- function(ind){
  dt2 <- dist_mat_list[[ind]]$hm
  a_h <- as.vector(dt2)
  
  ATE_high <- vector("double")
  pairs_high <- vector("double")
  
  for (percentage in seq){
    threshold <- quantile(a_h,percentage)
    
    n2_vec <- 1:nrow(dt2)
    list_2 <- lapply(n2_vec, cal_neighbour, df = dt2, thresh = threshold, y = hy, A = htr)
    mean_list_2 <- lapply(n2_vec,function(x) unlist(list_2[[x]][1]))
    mean_cal_2 <- unlist(mean_list_2)
    neighbour_list_2 <- lapply(n2_vec,function(x) unlist(list_2[[x]][2]))
    
    df_2 <- (data.frame(Y = hy, A = htr)
             %>%mutate(ind = row_number())
             %>%mutate(AAA = neighbour_list_2)
             %>%mutate(mean_cal = mean_cal_2)
             %>%filter(!is.na(mean_cal))
             %>%mutate(ATE = (Y-mean_cal)*ifelse(A==0,-1,1))
    )
    
    ATE_high <- append(ATE_high,mean(df_2$ATE))
    pairs_high <- append(pairs_high,sum(!is.na(unlist(neighbour_list_2)))/2)
  }
  
  return(list(ate=ATE_high,pair=pairs_high))
}
ind_mat <- 1:4
high_list <- lapply(ind_mat,get_ate_pair)
end_time <- Sys.time()
tm <- end_time - start_time
cat("Time for Propensity Matching High_Dim is:", tm, "minutes.")
```

### 4.5 Plotting Part for Low-Dim

```{r, figures-side, fig.show="hold", out.width="25%", warning=FALSE}
ATE_ps_low <- vector("double")
for (i in 1:4){
  ATE_low <- low_list[[i]]$ate
  pairs_low <- low_list[[i]]$pair
  ATE_ps_low <- append(ATE_ps_low,ATE_low[40:60])
  
  plot_low <- data.frame(x = seq, ATE = ATE_low, pairs = pairs_low)
  
  g_low <- ggplot(plot_low) +
    geom_point(aes(x,ATE)) +
    labs(
      title = paste0("ATE V.S. threshold No.",i),
      x = "threshold",
      y = "ATE"
    )
  print(g_low)
  
  match_low <- ggplot(plot_low) +
    geom_point(aes(x,pairs)) +
    labs(
      title = paste0("Pairs V.S. threshold No.",i),
      x = "threshold",
      y = "Pairs"
    )
  print(match_low)
}
cat("mean ATE =",mean(ATE_ps_low))
cat("std ATE =",sd(ATE_ps_low))
```

### 4.6 Plotting Part for High-Dim

```{r, fig.show="hold", out.width="25%", warning=FALSE}
ATE_ps_high <- vector("double")
for (i in ind_mat){
  ATE_high <- high_list[[i]]$ate
  pairs_high <- high_list[[i]]$pair
  ATE_ps_high <- append(ATE_ps_high,ATE_high[40:60])
  
  plot_high <- data.frame(x = seq, ATE = ATE_high, pairs = pairs_high)
  
  g_high <- ggplot(plot_high) +
    geom_point(aes(x,ATE)) +
    labs(
      title = paste0("ATE V.S. threshold No.",i),
      x = "threshold",
      y = "ATE"
    )
  print(g_high)
  
  match_high <- ggplot(plot_high)+
    geom_point(aes(x,pairs))+
    labs(
      title = paste0("Pairs V.S. threshold No.",i),
      x = "threshold",
      y = "Pairs"
    )
  print(match_high)
  
}
cat("mean ATE =",mean(ATE_ps_high))
cat("std ATE =",sd(ATE_ps_high))
```

From the plots above, we find that the result is unstable, and fluctuates randomly. Thus, we want to find some stable algorithm to estimate ATE.


















